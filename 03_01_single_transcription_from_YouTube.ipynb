{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/howsworkzco/avasgpt.github.io/blob/main/Chapter-3/03_01_single_transcription_from_YouTube.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDnv7vGjSIrp",
        "outputId": "e8391ebf-3f5d-43f5-ad35-4061a3fc426e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.4)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.12/dist-packages (15.0.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.6)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.4)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.13.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "#!pip install youtube-transcript-api==0.6.1 langchain==0.0.335 pytube==15.0.0\n",
        "!pip install -U langchain langchain-community youtube-transcript-api pytube\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import YoutubeLoader\n"
      ],
      "metadata": {
        "id": "6ryAXHD_SM5M"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=Q4OBx3S0Ysw\", add_video_info=True)\n",
        "\n",
        "#data = loader.load()\n",
        "#data[0].page_content\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "\n",
        "# Initialize with the correct 2026 import\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=Q4OBx3S0Ysw\",\n",
        "    add_video_info=False\n",
        ")\n",
        "\n",
        "# Load the data\n",
        "data = loader.load()\n",
        "print(data[0].page_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4P-WENfSQB8",
        "outputId": "f8829208-4c6f-41c8-d332-df845f1b96b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foreign [Applause] to perform Vector similarity search on images zillas is a vector database it is designed to handle massive data sets containing vectors vectors are just numerical representations of data so this is your text documents audio and that also includes images traditional methods of searching through these large data sets has been consuming and computationally expensive so zilis uses Advanced algorithms and data structures tailored specifically for Vector similarity search instead of comparing each point individually zilla's organizes these vectors in a way that optimizes similarity queries this allows you to quickly find items that are similar to a given query vector so is this focused on cutting-edge Technologies for data indexing storage and retrieval with an emphasis on GPU accelerated computing for a high level overview of this project we are going to get the connection from The Notebook to Melvis and set up a cluster we're going to import images from Google Drive I'll make sure to leave a link in the description we're going to set up zilla's Cloud we're going to insert our data and then we will do our similarity search using a resnet 50 model and then we're going to go back again and do it with resnet 152 to just compare the output and see how it's different to get started the first thing that you'll want to do is set yourself up azilla's account when you do that you will have a hundred dollars worth of free credits so that you'll be able to follow along with this demo for free I'm going to create a cluster and then we'll go over downloading the images creating the embeddings modeling those but then we'll be able to write those embeddings over to the milvis database I will be using the starter serverless plan and I'm just going to give this the name image search so that's the name of my collection the metric type there are two options and I'm going to be choosing this L2 which is just your standard euclidean distance all right so now I am going to put the public endpoint in my code I already have the API key over there copy this and you'll see that image search has been set up but that it doesn't actually have any data for us I'm going to add my URI [Music] like I said my API key is already here all right so now we're going to dive into this demo here is a Google Drive link that you can use to access the images that I'm using they are images of my family but if you want to go along with your own images that'll work too so we're starting like we normally do by pip installing our packages we'll restart our run time and then now I am just going to be setting up my directory to match the docs and so we are in the content folder and then I'll add one for python as well oh okay so now this should work all right so we are going to pip install a couple more libraries Pine milfus is going to be used to connect to zilla's Cloud we're going to use torch to run the embedding model we will use torch vision for the actual model and pre-processing and then G down is for working with Google Drive and the tqdm package is so that we get those cute little loading bars while our model is training then we will import these libraries here are the docs for starting here these docs are what I followed to put this together and so here you're going to need to add the Google Drive Link so you could use the link above open that up actually let me make sure that that works then we are going to set the output path and file name for the downloaded file and then download the file from the given Google Drive Link using gdam all right perfect so that link I gave you works next we're just going to be setting our parameters so we had given the collection name image search you had already seen me put my URI and I had already put my API key here and then now we need to set up our cloud so we will be first connecting to the zillas cloud cluster using the URI that we had there it's just if the collection already exists drop it okay and now we're going to be setting up our schema so it's going to have the ID the file path and the image embedding and we will now create an index on that collection here is that L2 euclidean distance metric that we already talked about and this is just to get the file path of the different images we have 1619 images okay so here we are going to create our base model using resnet 50. and this is actually not creating the model it is defining the model if you haven't used torch before I learned this the first time that I use torch is that they actually build their neural net models sequentially by adding the different layers so that's where we get this sequential function here we're creating a preprocessor for making sure the images are the same size so we're going to resize crop normalize or at least we're setting up a preprocessor so we didn't actually do those steps here but we created a function that will allow us to do that now we're going to insert the data so this one takes a minute to run but we're going to embed the function that embeds the batch and inserts it then we're going to read the images into batches for embedding and insertion and here we're actually going to insert the data all right I hope you're ready because we are about to perform our search now for this I have already placed four images that all looked different that we can do our search on the code here is going to go through and embed the images so this is the process of converting these photos into vectors using our resnet 50 model and we are going to iterate through and use that preprocessor that we set up that'll crop and normalize the images um then we're going to do the search through these embeddings for similar images and then at the end um it'll be using matplotlib to set up a visual display for us that will list the search time and the distance of the chosen searched photo to the image that I provided okay so let's get started all right perfect so we have our output and I am just going to store this so that we are able to do a comparison in a little bit once we have the output from our next model okay and now I'm just going to go through and actually update this to be using resnet 152 and rerun [Music] awesome now we get to go take a look at these results I actually went back and ran this a couple more times myself and found that in general the resnet 152 model had shorter search times or shorter distances but that was not always the case at all and even in our results we're going to see a lot of instances where the resnet 50 has shorter distances here though we will see that the resnet 152 model the search time was about three times faster than resnet 152. taking a look at the first photo we see that all the distances for the resnet 152 model are shorter compared to the resnet 50 model and really the only photo that looks different here is the last result in the second row of results the distances are actually larger with the resnet 152 model and you can really see this because the second result returned from the 152 results my daughter has her arms in and we'd assume that the ones that have the arms out are going to be more similar to the given photo in the third row of results it's my daughter and husband with their arms up wearing coats and the distances again they're higher with the resnet 152 model even though these are super close results and you get to benefit from the faster search time in the fourth row of results all the images look really similar with the exception of where is my son placed because you can't really see him in the given photo and then in the search results he's sort of in different places the distance for the first result is very close but then the distance for the resnet 152 model is much larger for the other two photos but again the search was quicker I hope you enjoyed this demo where we did Vector similarity search using images be sure to like And subscribe and I look forward to seeing you in my next demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "chunk_size = 1000,\n",
        "chunk_overlap = 50,\n",
        "length_function = len,\n",
        "add_start_index = True,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([data[0].page_content])\n",
        "\n",
        "## Inspect the different pieces of text\n",
        "print(texts[0])\n",
        "print(texts[1])\n",
        "print(texts[2])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6yCGAQoTNcl",
        "outputId": "8cfd151f-7273-42e9-dea4-b720318b5818"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='foreign [Applause] to perform Vector similarity search on images zillas is a vector database it is designed to handle massive data sets containing vectors vectors are just numerical representations of data so this is your text documents audio and that also includes images traditional methods of searching through these large data sets has been consuming and computationally expensive so zilis uses Advanced algorithms and data structures tailored specifically for Vector similarity search instead of comparing each point individually zilla's organizes these vectors in a way that optimizes similarity queries this allows you to quickly find items that are similar to a given query vector so is this focused on cutting-edge Technologies for data indexing storage and retrieval with an emphasis on GPU accelerated computing for a high level overview of this project we are going to get the connection from The Notebook to Melvis and set up a cluster we're going to import images from Google Drive I'll' metadata={'start_index': 0}\n",
            "page_content='going to import images from Google Drive I'll make sure to leave a link in the description we're going to set up zilla's Cloud we're going to insert our data and then we will do our similarity search using a resnet 50 model and then we're going to go back again and do it with resnet 152 to just compare the output and see how it's different to get started the first thing that you'll want to do is set yourself up azilla's account when you do that you will have a hundred dollars worth of free credits so that you'll be able to follow along with this demo for free I'm going to create a cluster and then we'll go over downloading the images creating the embeddings modeling those but then we'll be able to write those embeddings over to the milvis database I will be using the starter serverless plan and I'm just going to give this the name image search so that's the name of my collection the metric type there are two options and I'm going to be choosing this L2 which is just your standard' metadata={'start_index': 955}\n",
            "page_content='be choosing this L2 which is just your standard euclidean distance all right so now I am going to put the public endpoint in my code I already have the API key over there copy this and you'll see that image search has been set up but that it doesn't actually have any data for us I'm going to add my URI [Music] like I said my API key is already here all right so now we're going to dive into this demo here is a Google Drive link that you can use to access the images that I'm using they are images of my family but if you want to go along with your own images that'll work too so we're starting like we normally do by pip installing our packages we'll restart our run time and then now I am just going to be setting up my directory to match the docs and so we are in the content folder and then I'll add one for python as well oh okay so now this should work all right so we are going to pip install a couple more libraries Pine milfus is going to be used to connect to zilla's Cloud we're going' metadata={'start_index': 1902}\n"
          ]
        }
      ]
    }
  ]
}